{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a90109-72fa-403b-a711-e06ae0baec60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown('Process','audit',['audit', 'bona_fides_verification', 'capa_action', 'change_action', 'common', 'effectiveness_check', 'extension_request', 'organization', 'pharmaceutical_development_report', 'qualification', 'quality_event', 'schedule', 'scope_of_service'])\n",
    "dbutils.widgets.text('start_date','1990-09-10T23:58:01.000Z')\n",
    "dbutils.widgets.text('end_date','2024-15-00T23:58:01.000Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "956cc795-b83b-4b7b-939b-e7570f594e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process = dbutils.widgets.get('Process')\n",
    "start_date = dbutils.widgets.get('start_date')\n",
    "end_date = dbutils.widgets.get('end_date')\n",
    "print(process, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6f14d6-01ca-4d2d-a24d-c06265fa2ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a90f7880-112a-426f-80cf-a44602875123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "object_list = object_name[process]\n",
    "print(object_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2351eb0f-be3d-4268-a089-8865ad098809",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Source to Target files when column names are different"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data_val = None\n",
    "\n",
    "for object in object_list:\n",
    "#Defining Veeva to Read...   \n",
    "  df1 = spark.read.csv(f'dbfs:/FileStore/Project_name/{process}/{object}.csv',header=True,sep=',',escape='\"', multiLine=True)\n",
    "  df1 = df1.filter(f\"modified_date__v between '{start_date}' and '{end_date}'\")\n",
    "  print(object)\n",
    "  df1.createOrReplaceTempView('Veeva')\n",
    "  df2 = spark.sql(f\"select * from Veeva\")\n",
    "  veeva_object = 'Veeva_'+object\n",
    "  df2.createOrReplaceTempView(veeva_object)\n",
    "  veeva_cnt = df2.count()\n",
    "  print(veeva_cnt)\n",
    "\n",
    "#Defining S3 to Read...  \n",
    "  df3 = spark.read.csv(f's3://dev_bckt/inbound/project_name/{process}/{object}/delta/*',header=True,sep='|')\n",
    "  df3 = df3.filter(f\"modified_date__v between '{start_date}' and '{end_date}'\").distinct()\n",
    "  s3_temp_table = 'S3'\n",
    "  df3.createOrReplaceTempView(s3_temp_table)\n",
    "  s3_count_query = f\"select distinct * from {s3_temp_table} where (id,modified_date__v) in(select id,max(modified_date__v) from {s3_temp_table} group by id)\"\n",
    "  df4 = spark.sql(s3_count_query)\n",
    "  s3_object = 'S3_'+object\n",
    "  df4.createOrReplaceTempView(s3_object)\n",
    "  s3_cnt = df4.distinct().count()\n",
    "  #print(df4.columns)\n",
    "  if veeva_cnt == s3_cnt and veeva_cnt != 0:\n",
    "    count_status = 'Pass'\n",
    "  elif veeva_cnt == 0:\n",
    "    count_status = 'No Records'\n",
    "  else:\n",
    "    count_status = 'Fail'\n",
    "  #print(count_status)\n",
    "  \n",
    "#Defining Columns for Veeva and S3\n",
    "  veeva_columns = df2.columns\n",
    "  #print(veeva_columns)\n",
    "  veeva_cols = [cols for cols in veeva_columns if cols not in excluded_columns]\n",
    "  veeva_columns = [f\"substr(`{col}`,instr(`{col}`, '_')+1) as `{col}`\" if '.' in col and col.split('.')[1] == 'global_id' else f\"\"\"replace(replace(replace(replace(`{col}`, \"\\\\\\\\\", \"\\\\\\\\ \"),'\"',\"''\"),'\\\\n',' '),'\\\\t',' ') as `{col}`\"\"\" for col in veeva_cols]\n",
    "  \n",
    "  alias_veeva = []\n",
    "  for i in veeva_columns:\n",
    "    #alias_veeva_columns = f\"`{col}`\" + ' as ' + i\n",
    "    i = i.replace('site__c.global_id','site__c').replace('proactive_initiative__qdm.global_id','proactive_initiative__qdm') # columns rename\n",
    "    alias_veeva.append(i)\n",
    "  veeva_columns = ','.join(sorted(alias_veeva, key=lambda x: ('.' in x.split(' as ')[1], x.split(' as ')[1])))\n",
    "  #print(veeva_columns)\n",
    "\n",
    "  s3_columns = df4.columns\n",
    "  s3_columns = [f\"`{col}`\" for col in s3_columns if col not in excluded_columns]\n",
    "  alias_s3_columns = []\n",
    "  for c in s3_columns:\n",
    "    cols = c.replace('site__cr.id','site__c').replace('proactive_initiative__qdmr.id','proactive_initiative__qdm') # columns rename\n",
    "    alias_s3_columns.append(cols + ' as ' + c)\n",
    "  final_s3_columns = []\n",
    "  for v in alias_s3_columns:\n",
    "    x, y = v.split(\" as \")\n",
    "    interchange = f\"{y} as {x}\"\n",
    "    final_s3_columns.append(interchange)\n",
    "  sorted_list = sorted(final_s3_columns, key=lambda x: ('.' in x.split(' as ')[1],x.split(' as ')[1]))\n",
    "  s3_columns = ','.join(sorted_list)\n",
    "  #print(s3_columns)\n",
    "  \n",
    "#Data validation query and execution\n",
    "  veeva_m_s3_query = f\"select {veeva_columns} from {veeva_object} minus select {s3_columns} from {s3_object} where (id,modified_date__v) in(select id,max(modified_date__v) from {s3_object} group by id)\"\n",
    "  print(veeva_m_s3_query)\n",
    "  veeva_m_s3 = spark.sql(veeva_m_s3_query).count()\n",
    "  s3_m_veeva_query = f\"select {s3_columns} from {s3_object} where (id,modified_date__v) in(select id,max(modified_date__v) from {s3_object} group by id) minus select {veeva_columns} from {veeva_object}\"\n",
    "  s3_m_veeva = spark.sql(s3_m_veeva_query).count()\n",
    "\n",
    "#Comparing the data count of records in Veeva and S3\n",
    "  if veeva_m_s3 == 0 and s3_m_veeva == 0 and s3_cnt != 0:\n",
    "    data_val_status = 'Pass'\n",
    "  elif s3_cnt == 0:\n",
    "    data_val_status = 'No Records'\n",
    "  else:\n",
    "    data_val_status = 'Fail'\n",
    "\n",
    "#Creating a dataframe for displaying all the content  \n",
    "  if data_val is None:\n",
    "    data_val = spark.createDataFrame([(object, veeva_cnt, s3_cnt, count_status, veeva_m_s3, s3_m_veeva, data_val_status,veeva_m_s3_query,s3_m_veeva_query)], ['object', 'veeva_cnt', 's3_cnt', 'count_status', 'veeva_m_s3', 's3_m_veeva', 'data_val_status','veeva_m_s3_query','s3_m_veeva_query'])\n",
    "  else:\n",
    "    data_val = data_val.union(spark.createDataFrame([(object, veeva_cnt, s3_cnt, count_status, veeva_m_s3, s3_m_veeva, data_val_status,veeva_m_s3_query,s3_m_veeva_query)], ['object', 'veeva_cnt', 's3_cnt', 'count_status', 'veeva_m_s3', 's3_m_veeva', 'data_val_status','veeva_m_s3_query','s3_m_veeva_query']))\n",
    "\n",
    "data_val.display()\n",
    "veeva_s3_temp_table = \"veeva_s3_temp_table\"\n",
    "data_val.createOrReplaceTempView(veeva_s3_temp_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9086472d-2c28-4ea9-8e11-e83ba58c0747",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "File to Table when column names are same"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "s3_src_val = None\n",
    "\n",
    "for object in object_name[process]:\n",
    "  print(object)\n",
    "  df1 = spark.read.csv(f's3://dev_bckt/inbound/project_name/{process}/{object}/delta/*', header=True, inferSchema=False, sep='|')\n",
    "  df2 = df1.filter(f\"modified_date__v between '{start_date}' and '{end_date}'\")\n",
    "  s3_object = object\n",
    "  df2.createOrReplaceTempView(s3_object)\n",
    "  s3_cnt_query = f\"select distinct(*) from {s3_object}\"\n",
    "  s3_query = spark.sql(s3_cnt_query)\n",
    "  s3_cnt = s3_query.count()\n",
    "\n",
    "  s3_cols = df2.columns\n",
    "  renamed_columns = []\n",
    "  for col_name in s3_cols:\n",
    "      renamed_columns.append(f\"`{col_name}`\") #appending s3 columns which are renamed\n",
    "\n",
    "  s3_cols = ', '.join(sorted(renamed_columns))\n",
    "\n",
    "  s3_temptable = \"s3_\" + object\n",
    "  #displayHTML(s3_cols) # Displays s3 columns\n",
    "  s3_query.createOrReplaceTempView(s3_temptable)\n",
    "\n",
    "  src_count_query = \"\"\"select * from env_name.src_schema.{} where modified_date__v between '{}' AND '{}'\"\"\".format(object, start_date, end_date)\n",
    "  df3 = spark.sql(src_count_query)\n",
    "  src_temptable =\"src_\" + object\n",
    "  src_count = df3.count()\n",
    "  src_columns = df3.columns\n",
    "\n",
    "  src_audit_columns = ['edf_created_ts', 'edf_created_by', 'edf_updated_ts', 'edf_updated_by', 'edf_hash_key', 'edf_pipeline_name', 'edf_delete_flag']\n",
    "  src_columns = [col for col in src_columns if col not in src_audit_columns ]\n",
    "  src_columns =', '.join(sorted(src_columns))\n",
    "  #displayHTML(src_columns) # Displays src columns\n",
    "  df3.createOrReplaceTempView(src_temptable)\n",
    "\n",
    "  if s3_cnt !=0 and s3_cnt == src_count:\n",
    "    status = 'Pass'\n",
    "  elif s3_cnt == 0:\n",
    "    status = 'No Records'\n",
    "  else:\n",
    "    status = 'Fail'\n",
    "\n",
    "  s3_minus_src = f\"select distinct {s3_cols} from {s3_temptable} minus select {src_columns} from {src_temptable}\"\n",
    "  count_s3_minus_src = spark.sql(s3_minus_src).count()\n",
    "\n",
    "  src_minus_s3 = f\"select {src_columns} from {src_temptable} minus select {s3_cols} from {s3_temptable} where\"\n",
    "  count_src_minus_s3 = spark.sql(src_minus_s3).count()\n",
    "\n",
    "  if s3_cnt != 0 and count_s3_minus_src == 0 and count_src_minus_s3 ==0:\n",
    "    data_validation = 'Pass'\n",
    "  elif s3_cnt == 0:\n",
    "    data_validation = 'No Records'\n",
    "  else:\n",
    "    data_validation = 'Fail'\n",
    "\n",
    "  if s3_src_val is None:\n",
    "    s3_src_val = spark.createDataFrame([(object,s3_cnt,src_count, status,count_s3_minus_src,count_src_minus_s3, data_validation,s3_cnt_query,src_count_query,src_count_query,s3_minus_src,src_minus_s3)], ['object','s3_count','src_count', 'cnt_status','s3_minus_src','src_minus_s3', 'data_validation','s3_cnt_query','src_count_query','src_count_query','s3_minus_src_query','src_minus_s3_query'])\n",
    "  else:\n",
    "    s3_src_val = s3_src_val.union(spark.createDataFrame([(object,s3_cnt,src_count, status,count_s3_minus_src,count_src_minus_s3, data_validation,s3_cnt_query,src_count_query,src_count_query,s3_minus_src,src_minus_s3)], ['object','s3_count','src_count', 'cnt_status','s3_minus_src','src_minus_s3', 'data_validation','s3_cnt_query','src_count_query','src_count_query','s3_minus_src_query','src_minus_s3_query']))\n",
    "s3_src_val.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0968f84-6c44-4258-80db-a7701bcc7d61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table to Table when column names are same"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "src_pub_val = None\n",
    "\n",
    "for object in object_name[process]:\n",
    "  print(object)\n",
    "  src_count_query = \"\"\"select * from env_name.src_schema.{} where modified_date__v between '{}' AND '{}'\"\"\".format(object, start_date, end_date)\n",
    "  df3 = spark.sql(src_count_query)\n",
    "  src_temptable =\"src_\" + object\n",
    "  src_count = df3.count()\n",
    "  src_columns = df3.columns\n",
    "  src_columns =', '.join(sorted(src_columns))\n",
    "  df3.createOrReplaceTempView(src_temptable)\n",
    "\n",
    "  pub_count_query = \"\"\"select * from env_name.pub_schema.{} where modified_date__v between '{}' AND '{}'\"\"\".format(object, start_date, end_date)\n",
    "  df4 = spark.sql(pub_count_query)\n",
    "  pub_temptable =\"pub_\" + object\n",
    "  pub_count = df4.count()\n",
    "  pub_columns = df4.columns\n",
    "  pub_columns =', '.join(sorted(pub_columns))\n",
    "  #displayHTML(pub_columns) # Displays pub columns\n",
    "  df4.createOrReplaceTempView(pub_temptable)\n",
    "\n",
    "  if s3_cnt !=0 and src_count == pub_count:\n",
    "    status = 'Pass'\n",
    "  elif s3_cnt == 0:\n",
    "    status = 'No Records'\n",
    "  else:\n",
    "    status = 'Fail'\n",
    "\n",
    "  src_minus_pub = f\"select {src_columns} from {src_temptable} minus select {pub_columns} from {pub_temptable}\"\n",
    "  count_src_minus_pub = spark.sql(src_minus_pub).count()\n",
    "\n",
    "  pub_minus_src = f\"select {pub_columns} from {pub_temptable} minus select {src_columns} from {src_temptable}\"\n",
    "  count_pub_minus_src = spark.sql(pub_minus_src).count()\n",
    "\n",
    "  if s3_cnt != 0 and count_src_minus_pub == 0 and count_pub_minus_src ==0:\n",
    "    data_validation = 'Pass'\n",
    "  elif s3_cnt == 0:\n",
    "    data_validation = 'No Records'\n",
    "  else:\n",
    "    data_validation = 'Fail'\n",
    "\n",
    "  if src_pub_val is None:\n",
    "    src_pub_val = spark.createDataFrame([(object,src_count,pub_count, status,count_src_minus_pub,count_pub_minus_src, data_validation,src_count_query,pub_count_query,src_minus_pub,pub_minus_src)], ['object','src_count','pub_count', 'cnt_status','src_minus_pub','pub_minus_src', 'data_validation','src_count_query','pub_count_query','src_minus_pub_query','pub_minus_src_query'])\n",
    "  else:\n",
    "    src_pub_val = src_pub_val.union(spark.createDataFrame([(object,src_count,pub_count, status,count_src_minus_pub,count_pub_minus_src, data_validation,src_count_query,pub_count_query,src_minus_pub,pub_minus_src)], ['object','src_count','pub_count', 'cnt_status','src_minus_pub','pub_minus_src', 'data_validation','src_count_query','pub_count_query','src_minus_pub_query','pub_minus_src_query']))\n",
    "src_pub_val.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Automation when column names are different",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
